{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author: Aayush Mittal, Last edited:2/8/21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import LSTM, Activation, Dropout, Dense, Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "import string\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop,Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset and performing initial data preprocessing\n",
    "\n",
    "df = pd.read_csv('airline_sentiment_analysis.csv')\n",
    "df = df.loc[:,df.columns!=\"Unnamed: 0\"]\n",
    "df['airline_sentiment'] = df['airline_sentiment'].map({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class removes @mention, url, puntuation, digits and stop words\n",
    "\n",
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "        \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "\n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        return input_text.lower()\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    \n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating text and sentiment\n",
    "\n",
    "ct = CleanText()\n",
    "sr_clean = ct.transform(df.text)\n",
    "df.text=sr_clean\n",
    "\n",
    "text = df['text']\n",
    "\n",
    "texts = []\n",
    "for i in range(len(text)):\n",
    "  texts.append(text[i])\n",
    "\n",
    "y = df['airline_sentiment']\n",
    "y=y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11541.000000\n",
       "mean         9.307079\n",
       "std          3.773709\n",
       "min          1.000000\n",
       "25%          7.000000\n",
       "50%         10.000000\n",
       "75%         12.000000\n",
       "max         21.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lengths = df['text'].apply(lambda x: len(x.split(' ')))\n",
    "seq_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9970 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "max_words = 40000\n",
    "max_len = 21\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['airline_sentiment'])\n",
    "y = to_categorical(np.asarray(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.1, random_state = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/20\n",
      "325/325 [==============================] - 22s 13ms/step - loss: 0.4847 - accuracy: 0.8029 - val_loss: 0.3046 - val_accuracy: 0.8753\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.87532, saving model to best_model1.hdf5\n",
      "Epoch 2/20\n",
      "325/325 [==============================] - 4s 13ms/step - loss: 0.2642 - accuracy: 0.8982 - val_loss: 0.2510 - val_accuracy: 0.8970\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.87532 to 0.89697, saving model to best_model1.hdf5\n",
      "Epoch 3/20\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.2151 - accuracy: 0.9156 - val_loss: 0.2374 - val_accuracy: 0.8970\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.89697\n",
      "Epoch 4/20\n",
      "325/325 [==============================] - 4s 11ms/step - loss: 0.1907 - accuracy: 0.9266 - val_loss: 0.2333 - val_accuracy: 0.9117\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.89697 to 0.91169, saving model to best_model1.hdf5\n",
      "Epoch 5/20\n",
      "325/325 [==============================] - 4s 11ms/step - loss: 0.1820 - accuracy: 0.9286 - val_loss: 0.2289 - val_accuracy: 0.9065\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.91169\n",
      "Epoch 6/20\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.1740 - accuracy: 0.9344 - val_loss: 0.2282 - val_accuracy: 0.9091\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.91169\n",
      "Epoch 7/20\n",
      "325/325 [==============================] - 3s 11ms/step - loss: 0.1708 - accuracy: 0.9333 - val_loss: 0.2337 - val_accuracy: 0.9152\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.91169 to 0.91515, saving model to best_model1.hdf5\n",
      "Epoch 8/20\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 0.1681 - accuracy: 0.9358 - val_loss: 0.2212 - val_accuracy: 0.9126\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.91515\n",
      "Epoch 9/20\n",
      "325/325 [==============================] - 4s 13ms/step - loss: 0.1662 - accuracy: 0.9358 - val_loss: 0.2268 - val_accuracy: 0.9074\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.91515\n",
      "Epoch 10/20\n",
      "325/325 [==============================] - 5s 15ms/step - loss: 0.1580 - accuracy: 0.9412 - val_loss: 0.2239 - val_accuracy: 0.9100\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.91515\n",
      "Epoch 11/20\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.1559 - accuracy: 0.9384 - val_loss: 0.2225 - val_accuracy: 0.9126\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.91515\n",
      "Epoch 12/20\n",
      "325/325 [==============================] - 3s 11ms/step - loss: 0.1531 - accuracy: 0.9403 - val_loss: 0.2176 - val_accuracy: 0.9091\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.91515\n",
      "Epoch 13/20\n",
      "325/325 [==============================] - 4s 11ms/step - loss: 0.1437 - accuracy: 0.9439 - val_loss: 0.2256 - val_accuracy: 0.9100\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.91515\n",
      "Epoch 14/20\n",
      "325/325 [==============================] - 4s 11ms/step - loss: 0.1489 - accuracy: 0.9422 - val_loss: 0.2266 - val_accuracy: 0.9134\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.91515\n",
      "Epoch 15/20\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.1406 - accuracy: 0.9467 - val_loss: 0.2300 - val_accuracy: 0.9100\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.91515\n",
      "Epoch 16/20\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.1446 - accuracy: 0.9444 - val_loss: 0.2242 - val_accuracy: 0.9082\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.91515\n",
      "Epoch 17/20\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.1382 - accuracy: 0.9479 - val_loss: 0.2302 - val_accuracy: 0.9100\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.91515\n",
      "Epoch 18/20\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.1218 - accuracy: 0.9565 - val_loss: 0.2251 - val_accuracy: 0.9134\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.91515\n",
      "Epoch 19/20\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.1364 - accuracy: 0.9485 - val_loss: 0.2316 - val_accuracy: 0.9117\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.91515\n",
      "Epoch 20/20\n",
      "325/325 [==============================] - 3s 10ms/step - loss: 0.1204 - accuracy: 0.9560 - val_loss: 0.2278 - val_accuracy: 0.9160\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.91515 to 0.91602, saving model to best_model1.hdf5\n"
     ]
    }
   ],
   "source": [
    "#lstm model\n",
    "model1 = Sequential()\n",
    "model1.add(layers.Embedding(max_words, 21))\n",
    "model1.add(layers.LSTM(15,dropout=0.5))\n",
    "model1.add(layers.Dense(2,activation='softmax'))\n",
    "model1.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
    "checkpoint1 = ModelCheckpoint(\"best_model1.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "history = model1.fit(X_train, y_train, epochs=20,validation_data=(X_test, y_test),callbacks=[checkpoint1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/20\n",
      "325/325 [==============================] - 11s 18ms/step - loss: 0.4388 - accuracy: 0.8144 - val_loss: 0.2485 - val_accuracy: 0.8952\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.89524, saving model to best_model2.hdf5\n",
      "Epoch 2/20\n",
      "325/325 [==============================] - 5s 14ms/step - loss: 0.2157 - accuracy: 0.9155 - val_loss: 0.2271 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.89524 to 0.90563, saving model to best_model2.hdf5\n",
      "Epoch 3/20\n",
      "325/325 [==============================] - 5s 14ms/step - loss: 0.1848 - accuracy: 0.9271 - val_loss: 0.2367 - val_accuracy: 0.9039\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.90563\n",
      "Epoch 4/20\n",
      "325/325 [==============================] - 5s 14ms/step - loss: 0.1725 - accuracy: 0.9336 - val_loss: 0.2196 - val_accuracy: 0.9117\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.90563 to 0.91169, saving model to best_model2.hdf5\n",
      "Epoch 5/20\n",
      "325/325 [==============================] - 5s 14ms/step - loss: 0.1549 - accuracy: 0.9406 - val_loss: 0.2207 - val_accuracy: 0.9152\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.91169 to 0.91515, saving model to best_model2.hdf5\n",
      "Epoch 6/20\n",
      "325/325 [==============================] - 5s 14ms/step - loss: 0.1605 - accuracy: 0.9394 - val_loss: 0.2193 - val_accuracy: 0.9143\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.91515\n",
      "Epoch 7/20\n",
      "325/325 [==============================] - 5s 14ms/step - loss: 0.1418 - accuracy: 0.9492 - val_loss: 0.2214 - val_accuracy: 0.9134\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.91515\n",
      "Epoch 8/20\n",
      "325/325 [==============================] - 5s 14ms/step - loss: 0.1474 - accuracy: 0.9432 - val_loss: 0.2278 - val_accuracy: 0.9074\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.91515\n",
      "Epoch 9/20\n",
      "325/325 [==============================] - 5s 14ms/step - loss: 0.1363 - accuracy: 0.9518 - val_loss: 0.2186 - val_accuracy: 0.9203\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.91515 to 0.92035, saving model to best_model2.hdf5\n",
      "Epoch 10/20\n",
      "325/325 [==============================] - 5s 14ms/step - loss: 0.1240 - accuracy: 0.9553 - val_loss: 0.2164 - val_accuracy: 0.9134\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.92035\n",
      "Epoch 11/20\n",
      "325/325 [==============================] - 5s 15ms/step - loss: 0.1239 - accuracy: 0.9566 - val_loss: 0.2164 - val_accuracy: 0.9091\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.92035\n",
      "Epoch 12/20\n",
      "325/325 [==============================] - 5s 14ms/step - loss: 0.1240 - accuracy: 0.9517 - val_loss: 0.2200 - val_accuracy: 0.9152\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.92035\n",
      "Epoch 13/20\n",
      "325/325 [==============================] - 5s 14ms/step - loss: 0.1216 - accuracy: 0.9576 - val_loss: 0.2331 - val_accuracy: 0.9143\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.92035\n",
      "Epoch 14/20\n",
      "325/325 [==============================] - 5s 14ms/step - loss: 0.1147 - accuracy: 0.9568 - val_loss: 0.2364 - val_accuracy: 0.9100\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.92035\n",
      "Epoch 15/20\n",
      "325/325 [==============================] - 5s 15ms/step - loss: 0.1159 - accuracy: 0.9573 - val_loss: 0.2254 - val_accuracy: 0.9143\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.92035\n",
      "Epoch 16/20\n",
      "325/325 [==============================] - 5s 14ms/step - loss: 0.1113 - accuracy: 0.9604 - val_loss: 0.2515 - val_accuracy: 0.9195\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.92035\n",
      "Epoch 17/20\n",
      "325/325 [==============================] - 5s 14ms/step - loss: 0.1002 - accuracy: 0.9643 - val_loss: 0.2457 - val_accuracy: 0.9152\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.92035\n",
      "Epoch 18/20\n",
      "325/325 [==============================] - 5s 14ms/step - loss: 0.0981 - accuracy: 0.9654 - val_loss: 0.2415 - val_accuracy: 0.9117 0s - loss: 0.0972 - accuracy:  - ETA: 0s - loss:\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.92035\n",
      "Epoch 19/20\n",
      "325/325 [==============================] - 5s 14ms/step - loss: 0.0908 - accuracy: 0.9662 - val_loss: 0.2544 - val_accuracy: 0.9082\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.92035\n",
      "Epoch 20/20\n",
      "325/325 [==============================] - 5s 14ms/step - loss: 0.0929 - accuracy: 0.9659 - val_loss: 0.2364 - val_accuracy: 0.9160\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.92035\n"
     ]
    }
   ],
   "source": [
    "# bidirectional lstm model\n",
    "model2 = Sequential()\n",
    "model2.add(layers.Embedding(max_words, 40, input_length=max_len))\n",
    "model2.add(layers.Bidirectional(layers.LSTM(20,dropout=0.6)))\n",
    "model2.add(layers.Dense(2,activation='softmax'))\n",
    "model2.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
    "checkpoint2 = ModelCheckpoint(\"best_model2.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "history = model2.fit(X_train, y_train, epochs=20,validation_data=(X_test, y_test),callbacks=[checkpoint2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = keras.models.load_model(\"best_model2.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 - 1s - loss: 0.2186 - accuracy: 0.9203\n",
      "Model accuracy:  0.9203463196754456\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=2)\n",
    "print('Model accuracy: ',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = ['Negative','Positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing different examples\n",
    "sequence = tokenizer.texts_to_sequences(['this experience has been the worst , want my money back'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['this article is the best ever'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['i really loved how the technician helped me with the issue that i had'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
